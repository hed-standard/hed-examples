{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analyze the channels for the Attention Shift data\n",
    "\n",
    "This notebook assumes that a JSON dictionary called `originalChannels.json`\n",
    "has been created in the `\\code` subdirectory of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Log output:\n",
      "attention_shift_14_analyze_channels_log: Level None\n",
      "sub-001_task-AuditoryVisualShift_run-01_eeg.set:\n",
      "\t[ This key has 52 files with the same 36 channels]\n",
      "\t[ The channels are ['Fp1', 'Fpz', 'Fp2', 'AF3', 'AF4', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC6', 'FC2', 'T7', 'C3', 'Cz', 'C4', 'T8', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'PO3', 'PO4', 'O1', 'Oz', 'O2', 'LM', 'REye', 'LEye']]\n",
      "Overall:\n",
      "\t[ Dataset has 36 unique channels]\n",
      "\t[ {'REye', 'P4', 'PO3', 'Oz', 'T7', 'O1', 'Fpz', 'F3', 'Fp1', 'F4', 'CP5', 'C4', 'Fz', 'C3', 'P3', 'LM', 'AF3', 'FC5', 'F7', 'O2', 'CP1', 'Fp2', 'LEye', 'T8', 'CP2', 'P7', 'FC6', 'Cz', 'PO4', 'AF4', 'CP6', 'P8', 'Pz', 'FC2', 'FC1', 'F8'}]\n",
      "\n",
      "\n",
      "ERROR Summary:\n",
      "attention_shift_14_analyze_channels_log: Level ERROR\n",
      "sub-001_task-AuditoryVisualShift_run-01_eeg.set:\n",
      "Overall:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from hed.tools import get_file_list, HedLogger\n",
    "\n",
    "# Variables to set for the specific dataset\n",
    "bids_root_path = '/XXX/AttentionShiftWorkingPhaseTwo'\n",
    "exclude_dirs = ['sourcedata', 'stimuli', 'code']\n",
    "entities = ('sub', 'run')\n",
    "log_name = 'attention_shift_14_analyze_channels_log'\n",
    "channel_file = os.path.realpath(os.path.join(bids_root_path, \"code/channelsOriginal.json\"))\n",
    "\n",
    "# Set up the logger\n",
    "log_file_name = f\"code/curation_logs/{log_name}.txt\"\n",
    "logger = HedLogger(name=log_name)\n",
    "\n",
    "# Load the channels file\n",
    "with open(channel_file, 'r') as fp:\n",
    "    channel_info = json.load(fp)\n",
    "\n",
    "# Make the file list and dictionary for _events\n",
    "eeg_files = get_file_list(bids_root_path, extensions=[\".set\"], name_suffix=\"_eeg\", exclude_dirs=exclude_dirs)\n",
    "\n",
    "# Create the dictionaries of channels\n",
    "key_list = list(channel_info.keys())\n",
    "unique_dict = {}\n",
    "name_dict = {}\n",
    "for file_key, item in channel_info.items():\n",
    "    search_key = None\n",
    "    for skey in name_dict.keys():\n",
    "        if item == channel_info[skey]:\n",
    "            search_key = skey\n",
    "            break\n",
    "    if search_key:\n",
    "        search_values = name_dict[search_key]\n",
    "        name_dict[search_key].append(file_key)\n",
    "    else:\n",
    "        name_dict[file_key] = [file_key]\n",
    "\n",
    "# Now check the order of the elements in the list:\n",
    "all_channels = set\n",
    "for file_key, file_list in name_dict.items():\n",
    "    logger.add(file_key, f\"This key has {len(file_list)} files with the same {len(channel_info[file_key])} channels\")\n",
    "    logger.add(file_key, f\"The channels are {str(channel_info[file_key])}\")\n",
    "\n",
    "\n",
    "key_list = list(name_dict.keys())\n",
    "union_channels = set()\n",
    "inter_channels = set(channel_info[key_list[0]])\n",
    "for file_key in key_list:\n",
    "    union_channels = union_channels.union(set(channel_info[file_key]))\n",
    "    inter_channels = inter_channels.intersection(set(channel_info[file_key]))\n",
    "logger.add(\"Overall\", f\"Dataset has {len(union_channels)} unique channels\")\n",
    "logger.add(\"Overall\", f\"{str(union_channels)}\")\n",
    "\n",
    "# Output and save the log\n",
    "log_string = \"\\n\\nLog output:\\n\" + logger.get_log_string()\n",
    "error_string = \"\\n\\nERROR Summary:\\n\" + logger.get_log_string(level=\"ERROR\")\n",
    "print(log_string)\n",
    "print(error_string)\n",
    "save_path = os.path.join(bids_root_path, log_file_name)\n",
    "with open(save_path, \"w\") as fp:\n",
    "    fp.write(f\"{log_file_name} {datetime.datetime.now()}\\n\")\n",
    "    fp.write(log_string)\n",
    "    fp.write(error_string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create the template for the dataset\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}